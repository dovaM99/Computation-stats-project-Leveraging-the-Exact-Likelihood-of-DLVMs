# Project README â€” Leveraging the Exact Likelihood of DLVMs

> **Course:** Computational Statistics (M2)
> **Presentation date:** **7 January 2026**
> **Team:** 2 students â€¢ ~6h/week from now until the talk

---

## ğŸ“Œ Paper Under Study (to cite)

**Mattei, P.-A., & Frellsen, J. (2018). *Leveraging the Exact Likelihood of Deep Latent Variable Models*.** arXiv:1802.04826v4.
Key results: ill-posedness of ML for Gaussian-output DLVMs; spectral constraints that ensure bounded likelihood; data-dependent nonparametric upper bounds (NPMLE) yielding a **parsimony gap**; exact missing-data imputation via **Metropolis-within-Gibbs** (MWG) with the same computational budget as pseudo-Gibbs.    

> **How to cite in slides/report:**
> Mattei, P.-A., & Frellsen, J. (2018). Leveraging the Exact Likelihood of Deep Latent Variable Models. *arXiv:1802.04826*.

---

## ğŸ¯ Goals

1. Understand and present the **theory**: boundedness/ill-posedness, NPMLE bound, parsimony gap, convergence arguments.  
2. **Reproduce** minimal experiments: likelihood blow-up on Frey Faces; MWG vs pseudo-Gibbs for imputation (MNIST/OMNIGLOT/Caltech silhouettes).  
3. Deliver a clean **15-min talk** + Q&A with proof sketches and code-backed figures.

---

## ğŸ—ºï¸ Roadmap (Week-by-Week)

**Week 1 (Nov 11â€“17)** â€” *Big-picture pass + notes*

* Skim full paper; list main statements (Thm/Prop) and where theyâ€™re used in practice.
* Build a glossary: ELBO, amortization/approximation/parsimony gaps. 
* Output: **1-page plan** + glossary.

**Week 2 (Nov 18â€“24)** â€” *Ill-posedness & toy demo*

* Re-derive ill-posedness for Gaussian-output DLVMs; connect to **mode collapse**. 
* Toy 1D mixture: show likelihood blow-up as variance â†’ 0.
* Output: `02_blowup_toy.ipynb`.

**Week 3 (Nov 25â€“Dec 1)** â€” *Spectral constraints & boundedness*

* Prove boundedness with **Î£(z) âª° Î¾I**; implement variance clamp (logÏƒÂ² â‰¥ log Î¾) in a VAE. 
* Output: minimal PyTorch VAE (Gaussian decoder).

**Week 4 (Dec 2â€“8)** â€” *Nonparametric mixture upper bound*

* Study NPMLE upper bound `(GÌ‚)` and why the maximizer has **k â‰¤ n** components (Lindsay + compactification).  
* Approximate `(GÌ‚)` with constrained GaussianMixture (sklearn `reg_covar=Î¾`).
* Output: `03_sandwich_likelihood.ipynb` (ELBO â‰¤ log-lik â‰¤ NPMLE).

**Week 5 (Dec 9â€“15)** â€” *Parsimony gap & tightness (large-capacity)*

* Understand Theorem 3 and universal approximation requirement; make a clean proof sketch. 
* Ablation: vary decoder capacity `h` and observe parsimony gap.
* Output: figures for slides.

**Week 6 (Dec 16â€“22)** â€” *Missing-data imputation*

* Implement **pseudo-Gibbs** and **MWG**; re-derive acceptance ratio (eq. (15)).  
* Datasets: MNIST (bin), OMNIGLOT, Caltech silhouettes; MAR {40â€“80%}, top/bottom.
* Output: `04_imputation_mwg.ipynb` + F1 curves. 

**Week 7 (Dec 23â€“29)** â€” *Stabilize & document*

* Seed everything, freeze requirements, regenerate all figs.
* Write **â€œNotes de preuves & piÃ¨gesâ€** (2â€“3 pp).

**Week 8 (Dec 30â€“Jan 5)** â€” *Slides & dry-runs*

* Final slides + 15â€™ talk timing; prepare backup proofs/lemmas.
* Final repo cleanup.

**Jan 6 (Eve of talk)** â€” *Full rehearsal & pack release*

---

## ğŸ‘¥ Task Split (2 people)

**A â€” Theory lead**

* Ill-posedness & boundedness proofs; NPMLE existence (k â‰¤ n); parsimony gap tightness.    
* Draft â€œNotes de preuves & piÃ¨gesâ€ with figure sketches.

**B â€” Experiments lead**

* VAE (Gaussian decoder; clamp on variance), Frey Faces blow-up replication. 
* Sandwich bound with sklearn GMM; MWG vs pseudo-Gibbs imputation (+ Wilcoxon as in paper). 

> Weekly 1h cross-review (A reviews code; B reviews proofs).

---

## ğŸ§ª Mini-Project: â€œSandwich the Likelihoodâ€

**Goal:** Empirically bracket the **exact** log-likelihood between **ELBO** and **NPMLE** and study the **parsimony gap** across model capacities; then compare **MWG** vs **pseudo-Gibbs** for imputation at fixed compute.  

**Steps:**

1. Train VAE (Gaussian outputs) on Frey Faces with and without variance clamp; reproduce **likelihood blow-up** figure. 
2. Fit constrained GMM (reg_covar=Î¾) on training data to estimate `(GÌ‚)`; plot **ELBO**, **NPMLE bound**, gap. 
3. Do imputation on MNIST/OMNIGLOT/Caltech silhouettes with **pseudo-Gibbs** and **MWG** (same T, same VAE); report **F1 vs iterations** and **Wilcoxon** tests. 

**Deliverables:**

* `02_blowup_toy.ipynb`
* `03_sandwich_likelihood.ipynb`
* `04_imputation_mwg.ipynb`
* Figures mirroring paperâ€™s plots (blow-up, F1).  

---

## ğŸ§± Repo Structure

```
.
â”œâ”€â”€ README.md  (this file)
â”œâ”€â”€ notes/
â”‚   â””â”€â”€ 01_theory_notes.pdf   # proofs & pitfalls (2â€“3 pp)
â”œâ”€â”€ data/   # scripts to fetch/preprocess Frey Faces, MNIST, OMNIGLOT, Caltech silhouettes
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 02_blowup_toy.ipynb
â”‚   â”œâ”€â”€ 03_sandwich_likelihood.ipynb
â”‚   â””â”€â”€ 04_imputation_mwg.ipynb
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vae.py                # Gaussian decoder with variance clamp (Î¾)
â”œâ”€â”€ train.py                  # train VAE
â”œâ”€â”€ impute.py                 # pseudo-Gibbs & MWG samplers
â”œâ”€â”€ requirements.txt
â””â”€â”€ figs/                     # saved plots for slides
```

---

## ğŸ§° Implementation Notes (to match paper)

* **VAE training:** pathwise gradients (Roeder et al., 2017), Adam lr=1e-4, minibatch=10; Glorot init; TF Distributions or PyTorch equivalents. 
* **Frey Faces setup:** Gaussian DLVM, `d=5`, hidden `h=200`; same for inference net. 
* **Constrained GMM:** sklearn `GaussianMixture(reg_covar=Î¾)` to proxy `(GÌ‚)`. 
* **MWG acceptance (eq. 15):** ratio of importance ratios using the variational posterior as proposal; reduces to Gibbs if posterior is exact. 

---

## ğŸ–¥ï¸ Slide Outline (15â€™)

1. **Setup & ELBO refresher** (1.5â€™) â€” DLVMs, ELBO, encoder/decoder. 
2. **Ill-posedness vs boundedness** (3â€™) â€” Gaussian blow-up; Bernoulli well-posed; spectral constraint.  
3. **Nonparametric upper bound & parsimony gap** (3â€™) â€” `(GÌ‚)` with k â‰¤ n; gap definition; sandwiching inequality. 
4. **Tightness (large capacity)** (2â€™) â€” universal approximation â‡’ `(GÌ‚) â‰ˆ `(Î¸)`; proof sketch. 
5. **Imputation: pseudo-Gibbs vs MWG** (2.5â€™) â€” algorithm & acceptance ratio; results.  
6. **Our reproductions** (2.5â€™) â€” blow-up figure; sandwich curves; F1 curves.

---

## â“ Anticipated Questions

* **Where exactly is compactness used for the NPMLE?** â€” Lindsayâ€™s geometric likelihood with Alexandroff compactification for Gaussian outputs under spectral constraint. 
* **Why does blow-up relate to mode collapse?** â€” Infinite likelihood attained at near-singular covariances yields highly concentrated, poor generative models. 
* **Does MWG cost more?** â€” Essentially same compute as pseudo-Gibbs (plus cheap prior evals) but targets the **true** conditional; initialize with a few pseudo-Gibbs steps. 

---

## ğŸ“š Prerequisites & Related Reading (to cite)

* **Variational Inference / VAEs:**
  Kingma & Welling (2014); Burda et al. (2016); Cremer et al. (2018).  
* **Nonparametric Mixtures / NPMLE:**
  Lindsay (1983/1995); Hathaway (1985); van der Vaart & Wellner (1992); Kiefer & Wolfowitz (1956).  
* **MCMC / Imputation:**
  Metropolis (1953), Hastings (1970), Geman & Geman (1984), Gelman (1993). 

---

## âœ… Success Criteria

* Clear **proof sketches** aligned with the paperâ€™s arguments.
* Reproduced **blow-up** and **imputation** plots that qualitatively match the paper.  
* A compact **15-min** presentation with confident Q&A on convergence/compactness arguments.

---

## ğŸ”— Key Formulas/Objects (for quick reference)

* **ELBO:** `ELBO(Î¸,q) = â„“(Î¸) âˆ’ KL(q || p(Â·|X)) â‰¤ â„“(Î¸)` (eq. (4)). 
* **NPMLE bound:** `â„“(Î¸) â‰¤ â„“(GÌ‚)`; **parsimony gap** = `â„“(GÌ‚) âˆ’ â„“(Î¸)`. 
* **MWG acceptance (eq. 15):** ratio using variational posterior as proposal; equals 1 if posterior is exact. 

---

*This README provides the plan, division of labor, minimal code artifacts, and canonical citations to execute the project and deliver a rigorous talk on Mattei & Frellsen (2018).*

